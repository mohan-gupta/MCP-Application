import asyncio
from typing import Optional, Dict
from contextlib import AsyncExitStack

import json

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from openai import OpenAI

from client.cfg import gemini_api_key


class MCPClient:
    def __init__(self):
        # Initialize session and client objects
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        
        self.client = OpenAI(
            api_key=gemini_api_key,
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
        )
    
    async def connect_to_server(self, server_script_path: str):
        """Connect to an MCP server
        Args:
        	server_script_path: Path to the server script (.py or .js)
        """
        is_python = server_script_path.endswith('.py')
        is_js = server_script_path.endswith('.js')
        if not (is_python or is_js):
            raise ValueError("Server script must be a .py or .js file")
        
        command = "python" if is_python else "node"
        
        server_params = StdioServerParameters(
			command=command,
			args=[server_script_path],
			env=None
		)
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(self.stdio, self.write)
        )
        
        await self.session.initialize()
        
        # List available tools
        response = await self.session.list_tools()
        tools = response.tools
        print("\nConnected to server with tools:", [tool.name for tool in tools])
    
    def __get_llm_response(self, messages, tools) -> Dict:
        """
        Function to get the response from the llm
        Args:
			messages: List of message objects
			tools: List of tool specification objects
        """
        response = self.client.chat.completions.create(
            model="gemini-2.5-flash-lite",
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )
        
        return response.model_dump()
    
    async def run_tool(self, response_content):
        """
        Function to execute the tool
        Args:
        response_content: tool call response generated by the llm
        """
        # Get the tool name and its arguments
        tool_name = response_content["message"]["tool_calls"][0]["function"]["name"]
        tool_args = json.loads(response_content["message"]["tool_calls"][0]["function"]["arguments"])
        
        # Execute tool call
        result = await self.session.call_tool(tool_name, tool_args)
        
        return {
            "tool_name": tool_name,
            "tool_args": tool_args,
            "result": result
        }
    
    async def process_query(self, query: str) -> str:
        """
        Function to process the user query, if the query requires a tool,
        then this function will execute the required tool and
        use its output to generate final response 
        """
        messages = [
            {
                "role": "user",
                "content": query
            }
        ]
        
        # Get the tools
        tools_data = await self.session.list_tools()
        
        available_tools = [{
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.inputSchema
            }
        } for tool in tools_data.tools]
        
        # Make LLM API call
        response = self.__get_llm_response(messages=messages, tools=available_tools)
        
        agent_thoughts = []
        final_answer = ""
        
        # If llm response was final then return it
        if response["choices"][0]["finish_reason"] == "stop":
            final_answer = response["choices"][0]["message"]["content"]
            return {
                "thought": final_answer,
                "response": final_answer
            }
        
        # Process all the choices in the response and handle tool calls
        for choice in response["choices"]:
            
            # Execute tool call, if llm returns a tool call
            if choice["message"]["tool_calls"] is not None:
                tool_response = await self.run_tool(choice)
                agent_thoughts.append(
                    f"[Calling tool {tool_response["tool_name"]} with args {tool_response["tool_args"]}]"
                )
                
                # Add the messages in the chat histories
                if choice["message"]["content"] is not None:
                    messages.append({
                        "role": "assistant",
                        "content": choice["message"]["content"]
                    })                    
                
                # Add tool result in message 
                messages.append({
					"role": "assistant",
					"content": f"Observation: {tool_response["result"].content[0].text}"
     			})
                
                # Generate response using tool result
                response = self.__get_llm_response(messages=messages, tools=available_tools)
                agent_thoughts.append(response["choices"][0]["message"]["content"])
                
                if response["choices"][0]["finish_reason"] == "stop":
                    final_answer = response["choices"][0]["message"]["content"]
            
            # If choice is not a tool call then, add the response in final_text and assistant message
            else:
                agent_thoughts.append(choice["message"]["content"])
                messages.append({
					"role": "assistant",
					"content": choice["message"]["content"]
     			})
    
        return {
            "thought": "\n".join(agent_thoughts),
            "response": final_answer
        }
    
    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()


async def main(server_file_path: str):
    client = MCPClient()
    try:
        await client.connect_to_server(server_file_path)
    finally:
        await client.cleanup()

if __name__ == "__main__":
    server_file_path = "../server/server.py"
    asyncio.run(main(server_file_path))